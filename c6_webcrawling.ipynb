{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Web Crawler</h3>\n",
    "In general terms, the term\n",
    "<b>“crawler”</b> indicates a program’s ability to navigate web pages on its own, perhaps even\n",
    "without a well-defined end goal or purpose, endlessly exploring what a site or the web has\n",
    "to offer. While <b>“scraping”</b>  indicates a program that is extracting information from pages. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our first example, we’re going to work with the page at <a>http://www.\n",
    "webscrapingfordatascience.com/crawler/</a>. This page tries to emulate a simple\n",
    "numbers station (basically spit out a random list of numbers and links to\n",
    "direct you to a different page.),\n",
    "<br>\n",
    "Note that we’re using the urljoin function here. The reason why we do so is because\n",
    "the “href” attribute of links on the page refers to a relative URL, for instance, “?r=f01e7f\n",
    "02e91239a2003bdd35770e1173”, which we need to convert to an absolute one. We could\n",
    "just do this by prepending the base URL, that is, base_url + link_url, but once we’d\n",
    "start to follow links and pages deeper in the site’s URL tree, that approach would fail to\n",
    "work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?r=d494086c12004b50f5bcaf20dcd95cec\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "base_url = 'http://www.webscrapingfordatascience.com/crawler/'\n",
    "links_seen = set() #can't allow duplicates set like math set\n",
    "def visit(url, links_seen):\n",
    "    html = requests.get(url).text\n",
    "    html_soup = BeautifulSoup(html, 'html.parser')\n",
    "    links_seen.add(url)\n",
    "    for link in html_soup.find_all(\"a\"):\n",
    "        link_url = link.get('href')\n",
    "        if link_url is None:\n",
    "            continue\n",
    "        # print(link_url) #?r=d494086c12004b50f5bcaf20dcd95cec !realative path \n",
    "        full_url = urljoin(url, link_url)\n",
    "        if full_url in links_seen:\n",
    "            continue\n",
    "        print('Found a new page:', full_url)\n",
    "        # Normally, we'd store the results here too\n",
    "        visit(full_url, links_seen)\n",
    "visit(base_url, links_seen)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you run above script, you’ll see that it will start visiting the different URLs. If you let it\n",
    "run for a while, however, this script will certainly crash, and not only because of network\n",
    "hiccups. The reason for this is because we use <b>recursion</b>: the visit function is calling\n",
    "itself over and over again, without an opportunity to go back up in the call tree as every\n",
    "page will contain links to other pages. relying on recursion for web crawling is generally not a robust idea. We can\n",
    "rewrite our code as follows without recursion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "links_todo = ['http://www.webscrapingfordatascience.com/crawler/']\n",
    "links_seen = set()\n",
    "def visit(url, links_seen):\n",
    "    html = requests.get(url).text\n",
    "    html_soup = BeautifulSoup(html, 'html.parser')\n",
    "    new_links = []\n",
    "    for link in html_soup.find_all(\"a\"):\n",
    "        link_url = link.get('href')\n",
    "        if link_url is None:\n",
    "            continue\n",
    "        full_url = urljoin(url, link_url)\n",
    "        if full_url in links_seen:  \n",
    "            continue\n",
    "        # Normally, we'd store the results here too\n",
    "        new_links.append(full_url)\n",
    "    return new_links\n",
    "while links_todo:\n",
    "    url_to_visit = links_todo.pop()\n",
    "    links_seen.add(url_to_visit)\n",
    "    print('Now visiting:', url_to_visit)\n",
    "    new_links = visit(url_to_visit, links_seen)\n",
    "    print(len(new_links), 'new link(s) found')\n",
    "    links_todo += new_links"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above program is better, but it still has several\n",
    "drawbacks. If our program crashes (e.g., when your Internet connection or the website\n",
    "is down), you’ll have to restart from scratch again. Also, we have no idea how large the\n",
    "links_seen set might become. Normally, your computer will have plenty of memory\n",
    "available to easily store thousands of URLs, though we might wish to resort to a database\n",
    "to store intermediate progress information as well as the results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Storing Results in DB</h3>\n",
    "Let’s adapt our example to make it more robust against crashes by storing progress and\n",
    "result information in a database. We’re going to use the “records” library to manage an\n",
    "SQLite database (a file based though powerful database system) in which we’ll store our\n",
    "queue of links and retrieved numbers from the pages we crawl, which can be installed\n",
    "using pip:\n",
    "<i>pip install -U records</i>\n",
    "<br>\n",
    "<b>WORKED for Mysql</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "import mysql.connector as mysql\n",
    "from mysql.connector import Error #error library for error messages \n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "conn=mysql.connect(host='localhost',database='web_crawler',user='root',password='')\n",
    "db=conn.cursor()\n",
    "# db.execute('DROP TABLE IF EXISTS links')\n",
    "# db.execute('DROP TABLE IF EXISTS numbers')\n",
    "\n",
    "db.execute('''CREATE TABLE IF NOT EXISTS links (\n",
    "url varchar(500) PRIMARY KEY,\n",
    "created_at datetime,\n",
    "visited_at datetime NULL)''')\n",
    "db.execute('''CREATE TABLE IF NOT EXISTS numbers (url varchar(500), number integer,\n",
    "PRIMARY KEY (url, number))''')\n",
    "\n",
    "def store_link(url):\n",
    "    try:\n",
    "        db.execute('''INSERT INTO links (url, created_at)\n",
    "        VALUES (\\'%s\\', CURRENT_TIMESTAMP)'''%url)\n",
    "        conn.commit()\n",
    "    except Error as e:\n",
    "        print('Inserting URL',format(e))\n",
    "\n",
    "def get_random_unvisited_link():\n",
    "    db.execute('SELECT * FROM links WHERE visited_at IS NULL ORDER BY RAND() LIMIT 1')\n",
    "    data=db.fetchone()\n",
    "    url=data[0]\n",
    "    return None if data is None else url\n",
    "\n",
    "def store_number(url, number):\n",
    "    try:\n",
    "        db.execute('''INSERT INTO numbers (url, number)\n",
    "        VALUES (\\'%s\\', %d)'''%(url,number))\n",
    "    except Error as e: \n",
    "        print('Storing Number',format(e))\n",
    "\n",
    "def visit(url):\n",
    "    html = requests.get(url).text\n",
    "    html_soup = BeautifulSoup(html, 'html.parser')\n",
    "    new_links = []\n",
    "    for td in html_soup.find_all(\"td\"):\n",
    "        store_number(url, int(td.text.strip()))\n",
    "    for link in html_soup.find_all(\"a\"):\n",
    "        link_url = link.get('href')\n",
    "        if link_url is None:continue\n",
    "        full_url = urljoin(url, link_url)\n",
    "        new_links.append(full_url)\n",
    "    return new_links\n",
    "\n",
    "def mark_visited(url):\n",
    "    db.execute('''UPDATE links SET visited_at=CURRENT_TIMESTAMP\n",
    "WHERE url=\\'%s\\'''' %url)\n",
    "    conn.commit()\n",
    "\n",
    "store_link('http://www.webscrapingfordatascience.com/crawler/')\n",
    "url_to_visit = get_random_unvisited_link()\n",
    "\n",
    "while url_to_visit is not None:\n",
    "    print('Now visiting:', url_to_visit)\n",
    "    new_links = visit(url_to_visit)\n",
    "    print(len(new_links), 'new link(s) found')\n",
    "    for link in new_links:\n",
    "        store_link(link)\n",
    "    mark_visited(url_to_visit)\n",
    "    url_to_visit = get_random_unvisited_link()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now try to use the same framework in order to build a crawler for Wikipedia. Our\n",
    "plan here is to store page titles, as well as keep track of “(from, to)” links on each page,\n",
    "starting from the main page. Note that our database scheme looks a bit different here: <b> WORKED FOR mysql</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "import mysql.connector as mysql\n",
    "from mysql.connector import Error #error library for error messages \n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin,urldefrag\n",
    "conn=mysql.connect(host='localhost',database='wikipedia',user='root',password='')\n",
    "db=conn.cursor()\n",
    "# db.execute('DROP TABLE IF EXISTS links')\n",
    "# db.execute('DROP TABLE IF EXISTS numbers')\n",
    "base_url = 'https://en.wikipedia.org/wiki/'\n",
    "\n",
    "def create_tables():\n",
    "    db.execute('''CREATE TABLE IF NOT EXISTS pages\n",
    "(url varchar(500) PRIMARY KEY ,\n",
    "page_title text NULL,\n",
    "created_at datetime, \n",
    "visited_at datetime NULL)''')\n",
    "    db.execute('''CREATE TABLE IF NOT EXISTS links (\n",
    "url varchar(255) ,\n",
    "url_to varchar(255),\n",
    "PRIMARY KEY(url,url_to ))''')\n",
    "\n",
    "def store_page(url):\n",
    "    try:\n",
    "        db.execute('''INSERT INTO pages (url, created_at)\n",
    "        VALUES (\\'%s\\', CURRENT_TIMESTAMP)'''%url)\n",
    "        conn.commit()\n",
    "    except Error as e:\n",
    "        print('Inserting URL',format(e))\n",
    "\n",
    "def get_random_unvisited_page():\n",
    "    db.execute('SELECT * FROM pages WHERE visited_at IS NULL ORDER BY RAND() LIMIT 1')\n",
    "    data=db.fetchone()\n",
    "    url=data[0]\n",
    "    return None if data is None else url\n",
    "\n",
    "def set_title(url, page_title):\n",
    "    page_title=page_title.replace('\\'','')\n",
    "    print(page_title)\n",
    "    db.execute('UPDATE pages SET page_title=\\'%s\\' WHERE url=\\'%s\\''%(page_title,url))\n",
    "    conn.commit()\n",
    "\n",
    "def store_link(url, url_to):\n",
    "    try:\n",
    "        db.execute('''INSERT INTO links (url, url_to)\n",
    "        VALUES (\\'%s\\',\\'%s\\')'''%(url,url_to))\n",
    "        conn.commit()\n",
    "    except Error as ie:\n",
    "        print(format(ie))\n",
    "\n",
    "def set_visited(url):\n",
    "    db.execute('''UPDATE pages SET visited_at=CURRENT_TIMESTAMP\n",
    "    WHERE url=\\'%s\\''''%url)\n",
    "    conn.commit()\n",
    "\n",
    "def visit(url):\n",
    "    print('Now visiting:', url)\n",
    "    resp = requests.get(url)\n",
    "    html=resp.text\n",
    "    html_soup = BeautifulSoup(html, 'html.parser')\n",
    "    page_title = html_soup.find(id='firstHeading')\n",
    "    page_title = page_title.text if page_title else ''\n",
    "    print(' page title:', page_title)\n",
    "    print('Response: ',resp.url)\n",
    "    print(url)\n",
    "    print()\n",
    "    set_title(url, page_title)\n",
    "    for link in html_soup.find_all(\"a\"):\n",
    "        link_url = link.get('href')\n",
    "        if link_url is None:continue\n",
    "        full_url = urljoin(base_url, link_url)\n",
    "        # Remove the fragment identifier part\n",
    "        full_url = urldefrag(full_url)[0]\n",
    "        if not full_url.startswith(base_url):continue # This is an external link, skip\n",
    "        store_link(url, full_url)\n",
    "        store_page(full_url)\n",
    "    set_visited(url)\n",
    "\n",
    "create_tables()\n",
    "store_page(base_url)\n",
    "url_to_visit = get_random_unvisited_page()\n",
    "print(url_to_visit)\n",
    "while url_to_visit is not None:\n",
    "    visit(url_to_visit)\n",
    "    url_to_visit = get_random_unvisited_page()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above program we know in advance which information we want\n",
    "to get out of the pages (the page title and links, in this case). In case you don’t know\n",
    "beforehand what you’ll want to get out of crawled pages, you might want to split up the\n",
    "crawling and parsing of pages even further, for example, by storing a complete copy\n",
    "of the HTML contents that can be parsed by a second script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now visiting: https://en.wikipedia.org/wiki/Treasurer_of_the_United_States\n",
      "Now visiting: https://en.wikipedia.org/wiki/Freedman%27s_Bank_Building\n",
      "Now visiting: https://en.wikipedia.org/wiki/Illusion_of_Kate_Moss\n",
      "Now visiting: https://en.wikipedia.org/wiki/Michael_Hillegas\n",
      "Now visiting: https://en.wikipedia.org/wiki/National_Law_Enforcement_Officers_Memorial\n",
      "Now visiting: https://en.wikipedia.org/wiki/JSTOR_(identifier)\n",
      "Now visiting: https://en.wikipedia.org/wiki/National_Register_of_Historic_Places_listings_in_New_Hampshire\n",
      "Now visiting: https://en.wikipedia.org/wiki/Richmond,_New_Hampshire\n",
      "Now visiting: https://en.wikipedia.org/wiki/Special:RecentChangesLinked/JSTOR\n",
      "Now visiting: https://en.wikipedia.org/wiki/National_Register_of_Historic_Places_listings_in_Alaska\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: './downloads/httpsupload.wikimedia.orgwikipediacommonsthumbccaAlaska_Native_Brotherhood_Hall2C_Sitka_Camp_No._12C_Katlian_Street2C_Sitka2C_28Sitka_Borough2C_Alaska29.jpg220px-Alaska_Native_Brotherhood_Hall2C_Sitka_Camp_No._12C_Katlian_Street2C_Sitka2C_28Sitka_Borough2C_Alaska29.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [28], line 121\u001b[0m\n\u001b[0;32m    119\u001b[0m url_to_visit \u001b[39m=\u001b[39m get_random_unvisited_page()\n\u001b[0;32m    120\u001b[0m \u001b[39mwhile\u001b[39;00m url_to_visit \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 121\u001b[0m     visit(url_to_visit)\n\u001b[0;32m    122\u001b[0m     url_to_visit \u001b[39m=\u001b[39m get_random_unvisited_page()\n",
      "Cell \u001b[1;32mIn [28], line 107\u001b[0m, in \u001b[0;36mvisit\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m    105\u001b[0m     img_url \u001b[39m=\u001b[39m urljoin(base_url, img_url)\n\u001b[0;32m    106\u001b[0m     filename \u001b[39m=\u001b[39m url_to_file_name(img_url)\n\u001b[1;32m--> 107\u001b[0m     download(img_url, filename)\n\u001b[0;32m    108\u001b[0m     store_image(url, img_url, filename)\n\u001b[0;32m    110\u001b[0m \u001b[39m# Store the HTML contents\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [28], line 69\u001b[0m, in \u001b[0;36mdownload\u001b[1;34m(url, filename)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdownload\u001b[39m(url, filename):\n\u001b[0;32m     68\u001b[0m     r \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 69\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(file_store, filename), \u001b[39m'\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m the_image:\n\u001b[0;32m     70\u001b[0m         \u001b[39mfor\u001b[39;00m byte_chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m4096\u001b[39m\u001b[39m*\u001b[39m\u001b[39m4\u001b[39m):\n\u001b[0;32m     71\u001b[0m             the_image\u001b[39m.\u001b[39mwrite(byte_chunk)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: './downloads/httpsupload.wikimedia.orgwikipediacommonsthumbccaAlaska_Native_Brotherhood_Hall2C_Sitka_Camp_No._12C_Katlian_Street2C_Sitka2C_28Sitka_Borough2C_Alaska29.jpg220px-Alaska_Native_Brotherhood_Hall2C_Sitka_Camp_No._12C_Katlian_Street2C_Sitka2C_28Sitka_Borough2C_Alaska29.jpg'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import os, os.path\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urldefrag\n",
    "import mysql.connector as mysql\n",
    "from mysql.connector import Error #error library for error messages \n",
    "\n",
    "conn=mysql.connect(host='localhost',database='wikipedia_2',user='root',password='')\n",
    "db=conn.cursor()\n",
    "# db.execute('DROP TABLE IF EXISTS links')\n",
    "# db.execute('DROP TABLE IF EXISTS numbers')\n",
    "\n",
    "def create_tables():\n",
    "        # This table keeps track of crawled and to-crawl pages\n",
    "    db.execute('''CREATE TABLE IF NOT EXISTS pages (\n",
    "    url varchar(500) PRIMARY KEY,\n",
    "    created_at datetime,\n",
    "    html_file text NULL,\n",
    "    visited_at datetime NULL)''')\n",
    "    # This table keeps track of a-tags\n",
    "    db.execute('''CREATE TABLE IF NOT EXISTS links (\n",
    "    url varchar(255), link_url varchar(255),\n",
    "    PRIMARY KEY (url, link_url))''')\n",
    "    # This table keeps track of img-tags\n",
    "    db.execute('''CREATE TABLE IF NOT EXISTS images (\n",
    "    url varchar(255), img_url varchar(255), img_file text,\n",
    "    PRIMARY KEY (url, img_url))''')\n",
    "\n",
    "base_url = 'https://en.wikipedia.org/wiki/'\n",
    "file_store = './downloads/'\n",
    "\n",
    "\n",
    "if not os.path.exists(file_store):\n",
    "    os.makedirs(file_store)\n",
    "\n",
    "def store_page(url):\n",
    "    try:\n",
    "        db.execute('''INSERT INTO pages (url, created_at)\n",
    "        VALUES (\\'%s\\', CURRENT_TIMESTAMP)'''%url)\n",
    "        conn.commit()\n",
    "    except Error as e:\n",
    "        pass \n",
    "def get_random_unvisited_page():\n",
    "    db.execute('SELECT * FROM pages WHERE visited_at IS NULL ORDER BY RAND() LIMIT 1')\n",
    "    data=db.fetchone()\n",
    "    url=data[0]\n",
    "    return None if data is None else url\n",
    "\n",
    "def store_link(url, link_url):\n",
    "    try:\n",
    "        db.execute('''INSERT INTO links (url, link_url)\n",
    "        VALUES (\\'%s\\', \\'%s\\')'''%(url,link_url))\n",
    "        conn.commit()\n",
    "    except Error as ie:\n",
    "        pass\n",
    "\n",
    "def should_visit(link_url):\n",
    "    link_url = urldefrag(link_url)[0]\n",
    "    if not link_url.startswith(base_url):return None\n",
    "    return link_url\n",
    "\n",
    "def url_to_file_name(url):\n",
    "    url = str(url).strip().replace(' ', '_')\n",
    "    return re.sub(r'(?u)[^-\\w.]', '', url)\n",
    "\n",
    "def download(url, filename):\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(os.path.join(file_store, filename), 'wb') as the_image:\n",
    "        for byte_chunk in r.iter_content(chunk_size=4096*4):\n",
    "            the_image.write(byte_chunk)\n",
    "\n",
    "def store_image(url, img_url, img_file):\n",
    "    try:\n",
    "        db.execute('''INSERT INTO images (url, img_url, img_file)\n",
    "        VALUES (\\'%s\\', \\'%s\\', \\'%s\\')'''%(url,img_url,img_file))\n",
    "        conn.commit()\n",
    "    except Error as ie:\n",
    "        pass\n",
    "def set_visited(url, html_file):\n",
    "    db.execute('''UPDATE pages\n",
    "    SET visited_at=CURRENT_TIMESTAMP,\n",
    "    html_file=\\'%s\\'\n",
    "    WHERE \\'%s\\'\n",
    "    '''%(html_file,url))\n",
    "\n",
    "    conn.commit()\n",
    "def visit(url):\n",
    "    print('Now visiting:', url)\n",
    "    resp = requests.get(url)\n",
    "    html=resp.text\n",
    "    html_soup = BeautifulSoup(html, 'html.parser')\n",
    "    for link in html_soup.find_all(\"a\"):\n",
    "        link_url = link.get('href')\n",
    "        if link_url is None:continue\n",
    "        link_url = urljoin(base_url, link_url)\n",
    "        store_link(url, link_url)\n",
    "        full_url = should_visit(link_url)\n",
    "        if full_url:\n",
    "            # Queue for crawling\n",
    "            store_page(full_url)\n",
    "    for img in html_soup.find_all(\"img\"):\n",
    "        img_url = img.get('src')\n",
    "        if img_url is None:continue\n",
    "        img_url = urljoin(base_url, img_url)\n",
    "        filename = url_to_file_name(img_url)\n",
    "        download(img_url, filename)\n",
    "        store_image(url, img_url, filename)\n",
    "\n",
    "    # Store the HTML contents\n",
    "    filename = url_to_file_name(url) + '.html'\n",
    "    fullname = os.path.join(file_store, filename)\n",
    "    with open(fullname, 'w', encoding='utf-8') as the_html:\n",
    "        the_html.write(html)\n",
    "    set_visited(url, filename)\n",
    "    \n",
    "create_tables()   \n",
    "store_page(base_url)\n",
    "url_to_visit = get_random_unvisited_page()\n",
    "while url_to_visit is not None:\n",
    "    visit(url_to_visit)\n",
    "    url_to_visit = get_random_unvisited_page()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61abb1faae53f79e806d9c12619482227edf26d7ba3168cb9b69b001ff4947ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
