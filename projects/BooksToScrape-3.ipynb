{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Books to Scrape</h2>\n",
    "here we will scrape <a>http://books.toscrape.com</a> for book information such as title,image,price,stock_availablity,ratings and product description. Again we will be using dataset to store data in sqlite db but this code also accounts updates â€” so that we can run it multiple times without inserting duplicate\n",
    "records in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import dataset\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "db = dataset.connect(\n",
    "    'sqlite:///C://Users//Qamar Abbas//Desktop//Learning//pws4ds//books_scrap.db')\n",
    "base_url = 'http://books.toscrape.com/'\n",
    "\n",
    "# Scrape the pages in the catalogue\n",
    "url = base_url\n",
    "inp = input('Do you wish to re-scrape the catalogue (y/n)? ')\n",
    "\n",
    "\n",
    "def scrape_books(html_soup, url):\n",
    "    for book in html_soup.select('article.product_pod'):\n",
    "        # For now, we'll only store the books url\n",
    "        book_url = book.find('h3').find('a').get('href')\n",
    "        book_url = urljoin(url, book_url)\n",
    "        path = urlparse(book_url).path\n",
    "        book_id = path.split('/')[2]\n",
    "# Upsert tries to update first and then insert instead\n",
    "        db['books'].upsert({\n",
    "            'book_id': book_id,\n",
    "            'last_seen': datetime.now()\n",
    "        }, ['book_id'])\n",
    "\n",
    "\n",
    "def scrape_book(html_soup, book_id):\n",
    "    main = html_soup.find(class_='product_main')\n",
    "    book = {}\n",
    "    book['book_id'] = book_id\n",
    "    book['title'] = main.find('h1').get_text(strip=True)\n",
    "    book['price'] = main.find(class_='price_color').get_text(strip=True)\n",
    "    book['stock'] = main.find(class_='availability').get_text(strip=True)\n",
    "    book['rating'] = ' '.join(main.find(\n",
    "        class_='star-rating').get('class')).replace('star-rating', '').strip()\n",
    "    book['img'] = html_soup.find(class_='thumbnail').find('img').get('src')\n",
    "    desc = html_soup.find(id='product_description')\n",
    "    book['description'] = ''\n",
    "    if desc:\n",
    "        book['description'] = desc.find_next_sibling('p').get_text(strip=True)\n",
    "    info_table = html_soup.find(string='Product Information').find_next(\n",
    "        'table')  # tag after find() result\n",
    "    for row in info_table.find_all('tr'):\n",
    "        header = row.find('th').get_text(strip=True)\n",
    "        # Since we'll use the header as a column, clean it a bit\n",
    "        # to make sure SQLite will accept it\n",
    "        header = re.sub('[^a-zA-Z]+', '_', header)\n",
    "        value = row.find('td').get_text(strip=True)\n",
    "        book[header] = value\n",
    "    db['book_info'].upsert(book, ['book_id']) #create table db_info if not existed create tables by passing book keys as column names\n",
    "\n",
    "\n",
    "# Scrape the pages in the catalogue\n",
    "while True and inp == 'y':\n",
    "    print('Now scraping page:', url)\n",
    "    r = requests.get(url)\n",
    "    html_soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    scrape_books(html_soup, url)\n",
    "\n",
    "    # Is there a next page?\n",
    "    next_a = html_soup.select('li.next > a')\n",
    "    if not next_a or not next_a[0].get('href'):\n",
    "        break\n",
    "    url = urljoin(url, next_a[0].get('href'))\n",
    "\n",
    "# Now scrape book by book, oldest first\n",
    "books = db['books'].find(order_by=['last_seen'])  # select query\n",
    "for book in books:\n",
    "    book_id = book['book_id']\n",
    "    book_url = base_url + 'catalogue/{}'.format(book_id)\n",
    "    print('Now scraping book:', book_url)\n",
    "    r = requests.get(book_url)\n",
    "    r.encoding = 'utf-8'\n",
    "    html_soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    scrape_book(html_soup, book_id)\n",
    "    # Update the last seen timestamp\n",
    "    db['books'].upsert({\n",
    "        'book_id': book_id,\n",
    "        'last_seen': datetime.now()\n",
    "    }, ['book_id'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61abb1faae53f79e806d9c12619482227edf26d7ba3168cb9b69b001ff4947ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
