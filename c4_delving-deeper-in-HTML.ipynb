{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> HTTP POST method <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Date': 'Mon, 28 Nov 2022 15:36:33 GMT', 'Server': 'Apache/2.4.41 (Ubuntu)', 'Vary': 'Accept-Encoding', 'Content-Encoding': 'gzip', 'Content-Length': '235', 'Keep-Alive': 'timeout=5, max=100', 'Connection': 'Keep-Alive', 'Content-Type': 'text/html; charset=UTF-8'}\n",
      "{'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive', 'Content-Length': '56', 'Content-Type': 'application/x-www-form-urlencoded'}\n",
      "<html>\n",
      "\t<body>\n",
      "\n",
      "\n",
      "<h2>Thanks for submitting your information</h2>\n",
      "\n",
      "<p>Here's a dump of the form data that was submitted:</p>\n",
      "\n",
      "<pre>array(5) {\n",
      "  [\"name\"]=>\n",
      "  string(5) \"Seppe\"\n",
      "  [\"gender\"]=>\n",
      "  string(1) \"M\"\n",
      "  [\"pizza\"]=>\n",
      "  string(4) \"like\"\n",
      "  [\"haircolor\"]=>\n",
      "  string(5) \"brown\"\n",
      "  [\"comments\"]=>\n",
      "  string(0) \"\"\n",
      "}\n",
      "</pre>\n",
      "\n",
      "\n",
      "\t</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# execute a normal GET request before sending the information through a POST, though this is not even required.\n",
    "import requests\n",
    "url = 'http://www.webscrapingfordatascience.com/postform2/'\n",
    "# First perform a GET request\n",
    "r = requests.get(url)\n",
    "# Followed by a POST request\n",
    "formdata = {\n",
    "'name': 'Seppe',\n",
    "'gender': 'M',\n",
    "'pizza': 'like',\n",
    "'haircolor': 'brown',\n",
    "'comments': ''\n",
    "}\n",
    "r = requests.post(url, data=formdata)\n",
    "print(r.headers) #response\n",
    "print(r.request.headers) #request\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>To illustrate this, try navigating to http://www.\n",
    "webscrapingfordatascience.com/postform3/, fill out, and submit the form. Now try\n",
    "the same again but wait a minute or two before pressing “Submit my information.” The\n",
    "web page will inform you that “You waited too long to submit this information.” Let’s try\n",
    "submitting this form using requests:</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "\t<body>\n",
      "\n",
      "\n",
      "Are you trying to submit information from somewhere else?\n",
      "\n",
      "\t</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = 'http://www.webscrapingfordatascience.com/postform3/'\n",
    "# No GET request needed?\n",
    "formdata = {\n",
    "'name': 'Seppe',\n",
    "'gender': 'M',\n",
    "'pizza': 'like',\n",
    "'haircolor': 'brown',\n",
    "'comments': ''\n",
    "}\n",
    "r = requests.post(url, data=formdata)\n",
    "print(r.text)# Will show: Are you trying to submit information from somewhere else?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>PROBLEM</b> How does the server know that request is submitted from somewhere else in this case from python. The answer lies in this input tag as shown here: <pre>input type=\"hidden\" name=\"protection\" value=\"2c17abf5d5b4e326bea802600ff88405</pre>\n",
    "Following program shows how to incoporate this input in out form POST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "\t<body>\n",
      "\n",
      "\n",
      "You waited too long to submit this information. Try <a href=\"./\">again</a>.\n",
      "\n",
      "\t</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = 'http://www.webscrapingfordatascience.com/postform3/'\n",
    "formdata = {\n",
    "'name': 'Seppe',\n",
    "'gender': 'M',\n",
    "'pizza': 'like',\n",
    "'haircolor': 'brown',\n",
    "'comments': '',\n",
    "'protection': '2c17abf5d5b4e326bea802600ff88405'\n",
    "}\n",
    "r = requests.post(url, data=formdata)\n",
    "print(r.text)\n",
    "# Will show: You waited too long to submit this information. Try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>Assuming you waited a minute before running this piece of code, the web server will\n",
    "now reply with a message indicating that it doesn’t want to handle this request. Indeed,\n",
    "we can confirm (using our browser), that the “protection” field appears to change every\n",
    "time we refresh the page, seemingly randomly. To work our way around this, we have\n",
    "no other alternative but to first fetch out the form’s HTML source using a GET request,\n",
    "get the value for the “protection” field, and then use that value in the subsequent POST\n",
    "request.</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "url = 'http://www.webscrapingfordatascience.com/postform3/'\n",
    "# First perform a GET request\n",
    "r = requests.get(url)\n",
    "# Get out the value for protection\n",
    "html_soup = BeautifulSoup(r.text, 'html.parser')\n",
    "p_val = html_soup.find('input', attrs={'name': 'protection'}).get('value')\n",
    "# Then use it in a POST request\n",
    "formdata = {\n",
    "    'name': 'Seppe',\n",
    "    'gender': 'M',\n",
    "    'pizza': 'like',\n",
    "    'haircolor': 'brown',\n",
    "    'comments': '',\n",
    "    'protection': p_val\n",
    "}\n",
    "r = requests.post(url, data=formdata)\n",
    "print(r.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Problem</b><pre>If GET requests use URL parameters, and POST requests send data as part of the HTTP request body,\n",
    "why do we need to separate arguments when we can already indicate the type of request, by\n",
    "using either the requests.get or request.post method?  \n",
    "for example, if you encounter the “form” tag definition in a page’s source code:\n",
    "form action=\"submit.html?type=student\" method=\"post\"\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "\t<body>\n",
      "\n",
      "\n",
      "<h2>Thanks for submitting your information</h2>\n",
      "\n",
      "<p>Here's a dump of the form data that was submitted:</p>\n",
      "\n",
      "<pre>array(1) {\n",
      "  [\"name\"]=>\n",
      "  string(5) \"Seppe\"\n",
      "}\n",
      "</pre>\n",
      "\n",
      "\n",
      "\t</body>\n",
      "</html>\n",
      "\n",
      "http://www.webscrapingfordatascience.com/postform2/?name=Totally+Not+Seppe\n"
     ]
    }
   ],
   "source": [
    "url = 'http://www.webscrapingfordatascience.com/postform2/'\n",
    "paramdata = {'name': 'Totally Not Seppe'}\n",
    "formdata = {'name': 'Seppe'}\n",
    "r = requests.post(url, params=paramdata, data=formdata)\n",
    "print(r.text)\n",
    "print(r.request.url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling input types file\n",
    "<plaintext><form action=\"upload.php\" method=\"post\" enctype=\"multipart/form-data\">\n",
    "<input type=\"file\" name=\"profile_picture\">\n",
    "<input type=\"submit\" value=\"Upload your profile picture\">\n",
    "</form>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive', 'Content-Length': '9548', 'Content-Type': 'multipart/form-data; boundary=cadd04ed30cba9ec763afa7f1e911ddc'}\n",
      "{'Date': 'Wed, 14 Dec 2022 16:55:24 GMT', 'Server': 'Apache/2.4.41 (Ubuntu)', 'Vary': 'Accept-Encoding', 'Content-Encoding': 'gzip', 'Content-Length': '181', 'Keep-Alive': 'timeout=5, max=100', 'Connection': 'Keep-Alive', 'Content-Type': 'text/html; charset=UTF-8'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = 'http://www.webscrapingfordatascience.com/postform2/'\n",
    "formdata = {'name': 'Seppe'}\n",
    "filedata = {'profile_picture': open('Capture.jpg', 'rb')}\n",
    "r = requests.post(url, data=formdata, files=filedata)\n",
    "print(r.request.headers) #!request header\n",
    "print(r.headers) #?response header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>More on headers </h2>\n",
    "<b>PROBLEM</b> Modifying headers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems you are using a scraper!\n",
      "{'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = 'http://www.webscrapingfordatascience.com/usercheck/'\n",
    "r = requests.get(url)\n",
    "# Shows: It seems you are using a scraper\n",
    "print(r.text)\n",
    "#!HOw does it knows that we are using a scrapper\n",
    "print(r.request.headers) #?because of User-Agent header line. requests adds it automaticly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Modifying Headers</b> to blend in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome, normal user!\n",
      "{'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36  (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = 'http://www.webscrapingfordatascience.com/usercheck/'\n",
    "my_headers = {\n",
    "'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 ' + ' (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'\n",
    "}\n",
    "r = requests.get(url, headers=my_headers)\n",
    "print(r.text)\n",
    "print(r.request.headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Referer</b> (mispelling of referrer) Header\n",
    "<pre>Browsers will include this header to indicate the URL of the web page that\n",
    "linked to the URL being requested. Some websites will check this to prevent “deep links”\n",
    "from working. To test this out, navigate to http://www.webscrapingfordatascience.\n",
    "com/referercheck/ in your browser and click the “secret page” link. You’ll be linked to\n",
    "another page (http://www.webscrapingfordatascience.com/referercheck/secret.\n",
    "php) containing the text “This is a totally secret page.” Now try opening this URL directly\n",
    "in a new browser tab. You’ll see a message “Sorry, you seem to come from another web\n",
    "page” instead. The same happens in requests:</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Date': 'Fri, 02 Dec 2022 14:28:31 GMT', 'Server': 'Apache/2.4.41 (Ubuntu)', 'Content-Length': '45', 'Keep-Alive': 'timeout=5, max=100', 'Connection': 'Keep-Alive', 'Content-Type': 'text/html; charset=UTF-8'}\n",
      "Sorry, you seem to come from another web page\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = 'http://www.webscrapingfordatascience.com/referercheck/secret.php'\n",
    "r = requests.get(url)\n",
    "print(r.headers)\n",
    "print(r.text)\n",
    "# Shows: Sorry, you seem to come from another web page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>When encountering such checks in requests, we can simply spoof the “Referer” header as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a totally secret page\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = 'http://www.webscrapingfordatascience.com/referercheck/secret.php'\n",
    "my_headers = {\n",
    "'Referer': 'http://www.webscrapingfordatascience.com/referercheck/'\n",
    "}\n",
    "r = requests.get(url, headers=my_headers)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Redirection </h2>\n",
    "<pre>Open the page http://www.webscrapingfordatascience.com/redirect/ in your browser.\n",
    "You’ll see that you’re immediately sent to another page (“destination.php”). Now do\n",
    "the same again while inspecting the network requests in your browser’s developer\n",
    "tools (in Chrome, you should enable the “Preserve log” option to prevent Chrome from\n",
    "cleaning the log after the redirect happens). Note how two requests are being made by\n",
    "your browser: the first to the original URL, which now returns a 302 status code. This\n",
    "status code instructs your browser to perform a second request to the “destination.php”\n",
    "URL. How does the browser know what the URL should be? By inspecting the original\n",
    "URL’s response, you’ll note that there is now a “Location” response header present,\n",
    "which contains the URL to be redirected to</pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>PROBLEM</b> Note that we get the HTTP reply corresponding with the final destination (“you’ve\n",
    "been redirected here from another page!”). In most cases, this default behavior is quite\n",
    "helpful: requests is smart enough to “follow” redirects on its own when it receives 3XX\n",
    "status codes. But what if this is not what we want? What if we’d like to get the contents of the original page? This isn’t shown in the browser either, but there might be a relevant\n",
    "response content present. What if we want to see the contents of the “Location” and\n",
    "“SECRET-CODE” headers manually?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Hello, there -- you've been redirected here from another page!\n",
      "\n",
      "{'Date': 'Fri, 02 Dec 2022 16:08:23 GMT', 'Server': 'Apache/2.4.41 (Ubuntu)', 'Content-Length': '64', 'Keep-Alive': 'timeout=5, max=99', 'Connection': 'Keep-Alive', 'Content-Type': 'text/html; charset=UTF-8'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = 'http://www.webscrapingfordatascience.com/redirect/'\n",
    "r = requests.get(url)\n",
    "print(r.text)\n",
    "print(r.headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Prevent auto redirection from requests method</b> set  allow_redirects to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will be redirected... bye bye!\n",
      "{'Date': 'Fri, 02 Dec 2022 15:10:53 GMT', 'Server': 'Apache/2.4.41 (Ubuntu)', 'SECRET-CODE': '1234', 'Location': 'http://www.webscrapingfordatascience.com/redirect/destination.php', 'Content-Length': '34', 'Keep-Alive': 'timeout=5, max=100', 'Connection': 'Keep-Alive', 'Content-Type': 'text/html; charset=UTF-8'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = 'http://www.webscrapingfordatascience.com/redirect/'\n",
    "r = requests.get(url, allow_redirects=False)\n",
    "print(r.text)\n",
    "print(r.headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Authentication</h2>\n",
    "Finally, let’s take a closer look at the 401 (“Unauthorized”) status code, which\n",
    "seems to indicate that HTTP provides some sort of authentication mechanism. Indeed,\n",
    "the HTTP standard includes a number of authentication mechanisms, one of which\n",
    "can be seen by accessing the URL <pre>http://www.webscrapingfordatascience.com/\n",
    "authentication/</pre>. You’ll note that this site requests a username and password through\n",
    "your browser. If you press “Cancel,” you’ll note that the website responds with a 401\n",
    "(“Unauthorized”) result. Try refreshing the page and entering any username and\n",
    "password combination.<br>\n",
    "To encrypt username and password requests provides another\n",
    "means to do so, using the auth argument: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello myusername.\n",
      "You entered mypassword as your password.\n",
      "{'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive', 'Authorization': 'Basic bXl1c2VybmFtZTpteXBhc3N3b3Jk'}\n",
      "{'Date': 'Fri, 02 Dec 2022 19:44:37 GMT', 'Server': 'Apache/2.4.41 (Ubuntu)', 'Content-Length': '58', 'Keep-Alive': 'timeout=5, max=100', 'Connection': 'Keep-Alive', 'Content-Type': 'text/html; charset=UTF-8'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = 'http://www.webscrapingfordatascience.com/authentication/'\n",
    "r = requests.get(url, auth=('myusername', 'mypassword'))\n",
    "print(r.text)\n",
    "print(r.request.headers)\n",
    "print(r.headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Dealing with Cookies </h2>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now go over some examples to learn how we can deal with cookies\n",
    "in requests. The first example we’ll explore can be found at <a>http://www.\n",
    "webscrapingfordatascience.com/cookielogin/</a>. You’ll see a simple login page.\n",
    "After successfully logging in (you can use any username and password in this\n",
    "example), you’ll be able to access a secret page over at the website <a>http://www.\n",
    "webscrapingfordatascience.com/cookielogin/secret.php</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<html>\n",
      "\n",
      "<body>\n",
      "\n",
      "\n",
      "You are logged in, you can now see the <a href=\"secret.php\">secret page</a>.\n",
      "\n",
      "\n",
      "</body>\n",
      "\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = 'http://www.webscrapingfordatascience.com/cookielogin/'\n",
    "data={\n",
    "    'username':123124,\n",
    "    'password':1231241\n",
    "}\n",
    "r=requests.post(url,data=data)\n",
    "print(r.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try closing and reopening\n",
    "your browser (or just open an Incognito or Private Mode browser tab) and accessing\n",
    "the secret URL directly. You’ll see that the server detects that you’re not sending the\n",
    "right cookie information and blocks you from seeing the secret code. The same can be\n",
    "observed when trying to access this page directly using requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hmm... it seems you are not logged in\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "resp=requests.get('http://www.webscrapingfordatascience.com/cookielogin/secret.php')\n",
    "print(resp.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, we need to set and include a cookie. To do so, we’ll use a new argument,\n",
    "called cookies. Note that we could use the headers argument (which we’ve seen before)\n",
    "to include a “Cookie” header, but we’ll see that cookies is a bit easier to use, as requests\n",
    "will take care of formatting the header appropriately. We could fall back on our browser’s developer tools, and\n",
    "get the cookie from the request headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a secret code: 1234\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = 'http://www.webscrapingfordatascience.com/cookielogin/secret.php'\n",
    "my_cookies = {'PHPSESSID': '2s8s8pmuc6fo3jbb3nsrulqier'}\n",
    "r = requests.get(url, cookies=my_cookies)\n",
    "print(r.text)\n",
    "\n",
    "phpsessid=''' #!PHPSESSID?\n",
    "<b>PHPSESSID</b> We use the php scripting language to power our examples,\n",
    "so that the cookie name to identify a user’s session is named “phpSeSSiD”.\n",
    "Other websites might use “session,” “SeSSiOn_iD,” “session_id,” or any other\n",
    "name as well. Do note, however, that the value representing a session should\n",
    "be constructed randomly in a hard-to-guess manner. Simply setting a cookie\n",
    "“is_logged_in=true” or “logged_in_user=Seppe” would of course be very easy to\n",
    "guess and spoof'''\n",
    "\n",
    "#PROBLEM:However, if we’d want to use this scraper later on, this particular session identifier might \n",
    "# have been flushed and become invalid."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>SOLUTION</b> We hence need to resort to a more robust system as follows: we’ll first perform a\n",
    "POST request simulating a login, get out the cookie value from the HTTP response, and\n",
    "use it for the rest of our “session.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a secret code: 1234\n"
     ]
    }
   ],
   "source": [
    "url = 'http://www.webscrapingfordatascience.com/cookielogin/'\n",
    "# First perform a POST request\n",
    "r = requests.post(url, data={'username': 'dummy', 'password': '1234'})\n",
    "# Get the cookie value, either from\n",
    "# r.headers or r.cookies print(r.cookies)\n",
    "my_cookies = r.cookies\n",
    "# r.cookies is a RequestsCookieJar object which can also\n",
    "# be accessed like a dictionary. The following also works:\n",
    "my_cookies['PHPSESSID'] = r.cookies.get('PHPSESSID')\n",
    "# Now perform a GET request to the secret page using the cookies\n",
    "r = requests.get(url + 'secret.php', cookies=my_cookies)\n",
    "print(r.text)\n",
    "# Shows: This is a secret code: 1234"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> More complex login (and cookie) flows.</b> \n",
    "Above code won't work for this logic. The reason behind this is related to something we’ve seen before:\n",
    "requests will automatically follow HTTP redirect status codes, but the “Set-Cookie”\n",
    "response header is present in the response following the HTTP POST request, and not\n",
    "in the response for the redirected page. We’ll hence need to use the allow_redirects\n",
    "argument once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<RequestsCookieJar[<Cookie PHPSESSID=335q142ifhvats5d68q4s8c4dl for www.webscrapingfordatascience.com/>]>\n",
      "This is a secret code: 1234\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = 'http://www.webscrapingfordatascience.com/redirlogin/'\n",
    "# First perform a POST request -- do not follow the redirect\n",
    "r = requests.post(url, data={'username': 'dummy', 'password': '1234'},\n",
    "allow_redirects=False)\n",
    "# Get the cookie value, either from r.headers or r.cookies\n",
    "print(r.cookies)\n",
    "my_cookies = r.cookies\n",
    "# Now perform a GET request manually to the secret page using the cookies\n",
    "r = requests.get(url + 'secret.php', cookies=my_cookies)\n",
    "print(r.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final example, navigate to <a>http://www.webscrapingfordatascience.com/\n",
    "trickylogin/</a>. This site works in more or less the same way (explore it in your browser),\n",
    "though note that the form tag now includes an “action” attribute. We might hence\n",
    "change our code as follows:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn’t seem to work for this example. The reason for this is that this particular\n",
    "example also checks whether we’ve actually visited the login page, and are hence not\n",
    "only trying to directly submit the login information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "You should login first before accessing the protected page!\n",
      "<br><br>\n",
      "<form method=\"post\" action=\"index.php?p=login\">\n",
      "\tUsername: <input type=\"text\" name=\"username\"><br>\n",
      "\tPassword: <input type=\"password\" name=\"password\"><br>\n",
      "\t<input type=\"Submit\" value=\"Access the secret page\">\n",
      "</form>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = 'http://www.webscrapingfordatascience.com/trickylogin/'\n",
    "# First perform a POST request -- do not follow the redirect\n",
    "# Note that the ?p=login parameter needs to be set\n",
    "r = requests.post(url, params={'p': 'login'},\n",
    "data={'username': 'dummy', 'password': '1234'},\n",
    "allow_redirects=False)\n",
    "print(r.status_code)\n",
    "# Set the cookies\n",
    "my_cookies = r.cookies\n",
    "# Now perform a GET request manually to the secret page using the cookies\n",
    "r = requests.get(url, params={'p': 'protected'}, cookies=my_cookies)\n",
    "print(r.text)\n",
    "# Hmm... where is our secret code?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn’t seem to work for this example. The reason for this is that this particular\n",
    "example also checks whether we’ve actually visited the login page, and are hence not\n",
    "only trying to directly submit the login information. \n",
    "<b>This also does not seem to work yet. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You should login first before accessing the protected page!\n",
      "<br><br>\n",
      "<form method=\"post\" action=\"index.php?p=login\">\n",
      "\tUsername: <input type=\"text\" name=\"username\"><br>\n",
      "\tPassword: <input type=\"password\" name=\"password\"><br>\n",
      "\t<input type=\"Submit\" value=\"Access the secret page\">\n",
      "</form>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = 'http://www.webscrapingfordatascience.com/trickylogin/'\n",
    "# First perform a normal GET request to get the form\n",
    "r = requests.post(url)\n",
    "# Then perform the POST request -- do not follow the redirect\n",
    "r = requests.post(url, params={'p': 'login'},\n",
    "data={'username': 'dummy', 'password': '1234'},\n",
    "allow_redirects=False)\n",
    "# Set the cookies\n",
    "my_cookies = r.cookies\n",
    "# Now perform a GET request manually to the secret page using the cookies\n",
    "r = requests.get(url, params={'p': 'protected'}, cookies=my_cookies)\n",
    "print(r.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously,\n",
    "the way that the server would “remember” that we’ve seen the login screen is by setting\n",
    "a cookie, so we need to retrieve that cookie after the first GET request to get the session\n",
    "identifier at that moment: <b>Again, this fails... the reason for this (you can verify this as well in your browser) is\n",
    "that this site changes the session identifier after logging in as an extra security measure</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You should login first before accessing the protected page!\n",
      "<br><br>\n",
      "<form method=\"post\" action=\"index.php?p=login\">\n",
      "\tUsername: <input type=\"text\" name=\"username\"><br>\n",
      "\tPassword: <input type=\"password\" name=\"password\"><br>\n",
      "\t<input type=\"Submit\" value=\"Access the secret page\">\n",
      "</form>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = 'http://www.webscrapingfordatascience.com/trickylogin/'\n",
    "# First perform a normal GET request to get the form\n",
    "r = requests.post(url)\n",
    "# Set the cookies already at this point!\n",
    "my_cookies = r.cookies\n",
    "# Then perform the POST request -- do not follow the redirect\n",
    "# We already need to use our fetched cookies for this request!\n",
    "r = requests.post(url, params={'p': 'login'},\n",
    "data={'username': 'dummy', 'password': '1234'},\n",
    "allow_redirects=False,\n",
    "cookies=my_cookies)\n",
    "# Now perform a GET request manually to the secret page using the cookies\n",
    "r = requests.get(url, params={'p': 'protected'}, cookies=my_cookies)\n",
    "print(r.text)\n",
    "# Still no secret?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<RequestsCookieJar[<Cookie PHPSESSID=ovja5vqlqqt60v5otrqpppmibj for www.webscrapingfordatascience.com/>]>\n",
      "<RequestsCookieJar[<Cookie PHPSESSID=jodc4qon1qascoosmc41uaa6me for www.webscrapingfordatascience.com/>]>\n",
      "Here is your secret code: 3838.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = 'http://www.webscrapingfordatascience.com/trickylogin/'\n",
    "# First perform a normal GET request to get the form\n",
    "r = requests.post(url)\n",
    "# Set the cookies\n",
    "my_cookies = r.cookies\n",
    "print(my_cookies)\n",
    "# Then perform the POST request -- do not follow the redirect\n",
    "# Use the cookies we got before\n",
    "r = requests.post(url, params={'p': 'login'},\n",
    "data={'username': 'dummy', 'password': '1234'},\n",
    "allow_redirects=False,\n",
    "cookies=my_cookies)\n",
    "# We need to update our cookies again\n",
    "# Note that the PHPSESSID value will have changed\n",
    "my_cookies = r.cookies\n",
    "print(my_cookies)\n",
    "# Now perform a GET request manually to the secret page\n",
    "# using the updated cookies\n",
    "r = requests.get(url, params={'p': 'protected'}, cookies=my_cookies)\n",
    "print(r.text)\n",
    "# Shows: Here is your secret code: 3838."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Dealing with Sessions </h2>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll notice a few things going on here: first, we’re creating a requests.Session\n",
    "object and using it to perform HTTP requests, using the same methods (get, post) as\n",
    "above. The example now works, without us having to worry about redirects or dealing\n",
    "with cookies manually. <b>SESSIONS:</b> It\n",
    "specifies that various requests belong together — to the same session — and that\n",
    "requests should hence deal with cookies automatically behind the scenes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is your secret code: 3838.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = 'http://www.webscrapingfordatascience.com/trickylogin/'\n",
    "my_session = requests.Session() #fetches session \n",
    "r = my_session.post(url)\n",
    "r = my_session.post(url, params={'p': 'login'},\n",
    "data={'username': 'dummy', 'password': '1234'})\n",
    "r = my_session.get(url, params={'p': 'protected'})\n",
    "print(r.text)\n",
    "# Shows: Here is your secret code: 3838."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that sessions also offers an additional benefit apart from\n",
    "dealing with cookies: if you need to set global header fields, such as the “User-Agent”\n",
    "header, this can simply be done once instead of using the headers argument every time\n",
    "to make a request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'User-Agent': 'Chrome!', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive', 'Content-Length': '0'}\n",
      "{'User-Agent': 'Chrome!', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive', 'Cookie': 'PHPSESSID=s21tc01rr2679r8lhm8kfgqv8c'}\n",
      "{'User-Agent': 'Chrome!', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive', 'Cookie': 'PHPSESSID=93qrfo3v9pnf998o98i0qfsfvg'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = 'http://www.webscrapingfordatascience.com/trickylogin/'\n",
    "my_session = requests.Session()\n",
    "my_session.headers.update({'User-Agent': 'Chrome!'}) #create global session\n",
    "# All requests in this session will now use this User-Agent header:\n",
    "r = my_session.post(url)\n",
    "print(r.request.headers)\n",
    "r = my_session.post(url, params={'p': 'login'},\n",
    "data={'username': 'dummy', 'password': '1234'})\n",
    "print(r.request.headers)\n",
    "r = my_session.get(url, params={'p': 'protected'})\n",
    "print(r.request.headers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Binary, JSON, and Other Forms of Content </h2>So far, we’ve only used requests to fetch simple\n",
    "textual or HTML-based content, though remember that to render a web page, your web\n",
    "browser will typically fire off a lot of HTTP requests, including requests to fetch images.\n",
    "Additionally, files, like a PDF file, say, are also downloaded using HTTP requests.\n",
    "To explore how this works in requests, we’ll be using an image containing a lovely\n",
    "picture of a kitten at <a>http://www.webscrapingfordatascience.com/files/kitten.jpg</a>.\n",
    "You might be inclined to just use the following approach:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Binary Format (Image)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'http://www.webscrapingfordatascience.com/files/kitten.jpg'\n",
    "r = requests.get(url)\n",
    "print(r.text) #!OUTPUT CHARACTERS we’re downloading binary data now, which cannot be represented as Unicode text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the text attribute, we need to use content, which returns the contents\n",
    "of the HTTP response body as a Python bytes object, which you can then save to a file: <b>Don’t Print</b> it’s not a good idea to print out the r.content attribute, as the large amount of text may easily crash your python console window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'http://www.webscrapingfordatascience.com/files/kitten.jpg'\n",
    "r = requests.get(url)\n",
    "with open('image.jpg', 'wb') as my_file:\n",
    "    my_file.write(r.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, note that when using this method, content() Python will store the full file contents\n",
    "in memory before writing it to your file. When dealing with huge files, this can easily\n",
    "overwhelm your computer’s memory capacity. To tackle this, requests also allows to\n",
    "stream in a response by setting the stream argument to True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'http://www.webscrapingfordatascience.com/files/kitten.jpg'\n",
    "r = requests.get(url, stream=True)\n",
    "# You can now use r.raw\n",
    "# r.iter_lines\n",
    "# and r.iter_content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>Once you’ve indicated that you want to stream back a response, you can work with\n",
    "the following attributes and methods:\n",
    "<li>r.raw provides a file-like object representation of the response. This\n",
    "is not often used directly and is included for advanced purposes.</li>\n",
    "<li> The iter_lines method allows you to iterate over a content body line</li>\n",
    "by line. This is handy for large textual responses.\n",
    "<li> The iter_content method does the same for binary data </li> </ul>\n",
    "Let’s use iter_content to complete our example above:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'http://www.webscrapingfordatascience.com/files/kitten.jpg'\n",
    "r = requests.get(url, stream=True)\n",
    "with open('image.jpg', 'wb') as my_file:\n",
    "# Read by 4KB chunks\n",
    "    for byte_chunk in r.iter_content(chunk_size=4096): #content is divided into chunks and then write in file \n",
    "        my_file.write(byte_chunk)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>JSON format </h3>JSON (JavaScript Object Notation), a lightweight textual data interchange format that\n",
    "is both relatively easy for humans to read and write and easy for machines to parse\n",
    "and generate. It is based on a subset of the JavaScript programming language, but its\n",
    "usage has become so widespread that virtually every programming language is able to\n",
    "read and generate it. You’ll see this format used a lot by various web APIs these days to\n",
    "provide content messages in a structured way.  To explore an example, head over\n",
    "to <a>http://www.webscrapingfordatascience.com/jsonajax/</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This page shows a simple\n",
    "lotto number generator. Open your browser’s developer tools, and try pressing the Get lotto numbers” button a few times... By exploring the source code of the page, you’ll\n",
    "notice a few things going on:\n",
    "<ul>\n",
    "<li>Even though there’s a button on this page, it is not wrapped by a\n",
    "form tag.</li>\n",
    "<li>When pressing the button, part of the page is updated without\n",
    "completely reloading the page</li>\n",
    "<li>The “Network” tab in Chrome will show that HTTP POST requests are\n",
    "being made when pressing the button</li>\n",
    "<li>You’ll notice a piece of code in the page source wrapped inside\n",
    "script tags</li>\n",
    "</ul>\n",
    "This page uses JavaScript inside the script tags to perform so-called AJAX\n",
    "requests. AJAX stands for Asynchronous JavaScript And XML. lets look at the\n",
    "HTTP requests it is making to see how it works:\n",
    "<ul>\n",
    "<li>POST requests are being made to “results.php”</li>\n",
    "<li>The “Content-Type” header is set to “application/x-www-formurlencoded,” just like before. The client-side JavaScript will make sure\n",
    "to reformat a JSON string to a an encoded equivalent.</li>\n",
    "<li>An “api_code” is submitted in the POST request body</li>\n",
    "<li>The HTTP response has a “Content-Type” header set to\n",
    "“application/json,” instructing the client to interpret the result as\n",
    "JSON data</li>\n",
    "</ul>\n",
    "We can just use text\n",
    "as before and, for example, convert the returned result to a Python structure manually\n",
    "(Python provides a json module to do so), but requests also provides a helpful json\n",
    "method to do this in one go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive', 'Content-Length': '16', 'Content-Type': 'application/x-www-form-urlencoded'}\n",
      "{'results': [1, 12, 13, 15, 24, 27, 29]}\n",
      "[1, 12, 13, 15, 24, 27, 29]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = 'http://www.webscrapingfordatascience.com/jsonajax/results.php'\n",
    "r = requests.post(url, data={'api_code': 'C123456'})\n",
    "print(r.request.headers)\n",
    "print(r.json())\n",
    "print(r.json().get('results'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There’s one important remark here, however. Some APIs and sites will also use an\n",
    "“application/json” “Content-Type” for formatting the request and hence submit the\n",
    "POST data as plain JSON. for instance <a>http://www.webscrapingfordatascience.com/jsonajax/results2.php</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive', 'Content-Length': '23', 'Content-Type': 'application/json'}\n",
      "{'results': [1, 3, 4, 14, 16, 22, 29]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = 'http://www.webscrapingfordatascience.com/jsonajax/results2.php'\n",
    "# Use the json argument to encode the data as JSON:\n",
    "r = requests.post(url, json={'api_code': 'C123456'})\n",
    "# Note the Content-Type header in the request:\n",
    "print(r.request.headers)\n",
    "print(r.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61abb1faae53f79e806d9c12619482227edf26d7ba3168cb9b69b001ff4947ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
